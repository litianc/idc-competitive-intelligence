"""
ç½‘ç«™ç»“æ„åˆ†æå·¥å…·

å¸®åŠ©åˆ†æç›®æ ‡ç½‘ç«™çš„HTMLç»“æ„ï¼Œæ‰¾å‡ºæ–‡ç« åˆ—è¡¨çš„é€‰æ‹©å™¨
"""

import requests
from bs4 import BeautifulSoup
import sys


def analyze_website(url):
    """åˆ†æç½‘ç«™ç»“æ„"""
    print(f"\n{'=' * 70}")
    print(f"åˆ†æç½‘ç«™: {url}")
    print("=" * 70)

    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = "utf-8"

        if response.status_code != 200:
            print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
            return

        print(f"âœ… é¡µé¢åŠ è½½æˆåŠŸ\n")

        soup = BeautifulSoup(response.text, "html.parser")

        # åˆ†æå¯èƒ½çš„æ–‡ç« åˆ—è¡¨å®¹å™¨
        print("ğŸ“‹ å¯èƒ½çš„æ–‡ç« åˆ—è¡¨å®¹å™¨:\n")

        # æŸ¥æ‰¾å¸¸è§çš„åˆ—è¡¨å®¹å™¨
        common_selectors = [
            ("ul.news-list", "æ–°é—»åˆ—è¡¨(ul)"),
            ("div.news-list", "æ–°é—»åˆ—è¡¨(div)"),
            ("ul.article-list", "æ–‡ç« åˆ—è¡¨(ul)"),
            ("div.article-list", "æ–‡ç« åˆ—è¡¨(div)"),
            ("div.list", "é€šç”¨åˆ—è¡¨"),
            ("ul.list", "é€šç”¨åˆ—è¡¨(ul)"),
        ]

        found_containers = []

        for selector, description in common_selectors:
            elements = soup.select(selector)
            if elements:
                found_containers.append((selector, len(elements), description))
                print(f"  âœ“ {selector:30s} - æ‰¾åˆ° {len(elements)} ä¸ª ({description})")

        # æŸ¥æ‰¾åŒ…å«å¤šä¸ªé“¾æ¥çš„div/ul
        print("\nğŸ“¦ åŒ…å«å¤šä¸ªé“¾æ¥çš„å®¹å™¨:\n")

        divs_with_links = []
        for div in soup.find_all(["div", "ul", "ol"]):
            links = div.find_all("a", recursive=False)
            if len(links) >= 3:  # è‡³å°‘3ä¸ªé“¾æ¥
                classes = " ".join(div.get("class", []))
                div_id = div.get("id", "")
                selector = f"{div.name}"
                if div_id:
                    selector += f"#{div_id}"
                if classes:
                    selector += f".{classes.split()[0]}"

                divs_with_links.append((selector, len(links)))

        # æŒ‰é“¾æ¥æ•°é‡æ’åº
        divs_with_links.sort(key=lambda x: -x[1])

        for selector, link_count in divs_with_links[:10]:
            print(f"  â€¢ {selector:40s} - {link_count} ä¸ªé“¾æ¥")

        # åˆ†ææ‰€æœ‰é“¾æ¥
        print(f"\nğŸ”— é¡µé¢é“¾æ¥åˆ†æ:\n")

        all_links = soup.find_all("a", href=True)
        print(f"  æ€»é“¾æ¥æ•°: {len(all_links)}")

        # åˆ†æé“¾æ¥æ¨¡å¼
        internal_links = []
        for link in all_links:
            href = link.get("href", "")
            text = link.get_text(strip=True)

            # å¯èƒ½æ˜¯æ–‡ç« é“¾æ¥
            if text and len(text) > 10 and (".html" in href or "/article/" in href or "/news/" in href):
                internal_links.append({
                    "text": text[:60],
                    "href": href,
                    "parent": link.parent.name if link.parent else ""
                })

        if internal_links:
            print(f"\n  å¯èƒ½çš„æ–‡ç« é“¾æ¥ (å‰10ä¸ª):\n")
            for i, link in enumerate(internal_links[:10], 1):
                print(f"    [{i}] {link['text']}")
                print(f"        URL: {link['href']}")
                print(f"        çˆ¶å…ƒç´ : <{link['parent']}>")
                print()

        # åˆ†ææ—¶é—´å…ƒç´ 
        print("â° å¯èƒ½çš„æ—¶é—´å…ƒç´ :\n")

        time_patterns = [
            "span.time",
            "span.date",
            "div.date",
            "time",
            ".publish-time",
            ".update-time"
        ]

        for pattern in time_patterns:
            elements = soup.select(pattern)
            if elements:
                example = elements[0].get_text(strip=True)
                print(f"  âœ“ {pattern:25s} - æ‰¾åˆ° {len(elements)} ä¸ª, ç¤ºä¾‹: {example}")

        print(f"\n{'=' * 70}")
        print("ğŸ’¡ å»ºè®®çš„é…ç½®:")
        print("=" * 70)

        if found_containers:
            selector, count, desc = found_containers[0]
            print(f"\næ–‡ç« åˆ—è¡¨å®¹å™¨: {selector}")

        if internal_links:
            print(f"æ–‡ç« é“¾æ¥æ•°é‡: {len(internal_links)}")
            print(f"\néœ€è¦åœ¨æµè§ˆå™¨ä¸­æ£€æŸ¥:")
            print(f"1. æ–‡ç« åˆ—è¡¨çš„å…·ä½“CSSç±»å")
            print(f"2. æ¯ç¯‡æ–‡ç« çš„å®¹å™¨å…ƒç´ ")
            print(f"3. æ ‡é¢˜ã€æ—¥æœŸã€é“¾æ¥çš„é€‰æ‹©å™¨")

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python analyze_website.py <URL>")
        print("\nç¤ºä¾‹:")
        print("  python analyze_website.py https://www.idcquan.com/news/")
        print("  python analyze_website.py https://www.dcworld.cn/")
        sys.exit(1)

    url = sys.argv[1]
    analyze_website(url)
