"""
Multi-source integrated collection test
Tests all 4 active media sources
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.scrapers.generic_scraper import ScraperFactory
from src.storage.database import Database
from datetime import datetime, date
import json
import re


class SimpleScorer:
    """Article scoring system (4-dimension model)"""

    def score_article(self, title, content, publish_date, source_tier):
        """Score article using 4 dimensions"""
        # 1. Business Relevance (40 points max)
        keywords = ["IDC", "æ•°æ®ä¸­å¿ƒ", "äº‘è®¡ç®—", "AI", "ç®—åŠ›", "GPU", "èŠ¯ç‰‡", "æ™ºç®—", "æœåŠ¡å™¨", "å¤§æ¨¡å‹"]
        text = title + " " + content
        keyword_count = sum(1 for kw in keywords if kw in text)
        relevance = min(40, keyword_count * 8)

        # 2. Timeliness (25 points max)
        if isinstance(publish_date, date):
            days_ago = (date.today() - publish_date).days
            timeliness = max(0, int(25 * (1 - days_ago / 7)))
        else:
            timeliness = 0

        # 3. Impact Scope (20 points max)
        impact = 0
        funding_patterns = [r"(\d+)äº¿.*?èèµ„", r"èèµ„.*?(\d+)äº¿", r"æŠ•èµ„.*?(\d+)äº¿"]
        for pattern in funding_patterns:
            match = re.search(pattern, text)
            if match:
                amount = float(match.group(1))
                if amount >= 10:
                    impact = 20
                elif amount >= 5:
                    impact = 15
                elif amount >= 1:
                    impact = 10
                break

        if not impact:
            high_impact_keywords = ["æ ‡å‡†", "è§„èŒƒ", "æ”¿ç­–", "çªç ´", "PUE", "å‘å¸ƒ", "ä¸Šå¸‚"]
            if any(kw in text for kw in high_impact_keywords):
                impact = 15

        # 4. Source Credibility (15 points max)
        credibility_map = {1: 15, 2: 8, 3: 3}
        credibility = credibility_map.get(source_tier, 3)

        total_score = relevance + timeliness + impact + credibility

        # Map to priority
        if total_score >= 70:
            priority = "é«˜"
        elif total_score >= 40:
            priority = "ä¸­"
        else:
            priority = "ä½"

        return {
            "total_score": total_score,
            "priority": priority,
            "dimension_scores": {
                "relevance": relevance,
                "timeliness": timeliness,
                "impact": impact,
                "credibility": credibility,
            },
        }


class SimpleClassifier:
    """Article classification system"""

    def classify(self, title, content):
        """Classify article"""
        text = title + " " + content

        categories = {
            "æŠ•èµ„": ["èèµ„", "æŠ•èµ„", "å¹¶è´­", "æ”¶è´­", "èµ„æœ¬", "äº¿å…ƒ", "IPO", "ä¸Šå¸‚"],
            "æŠ€æœ¯": ["GPU", "èŠ¯ç‰‡", "æ¶²å†·", "æŠ€æœ¯", "çªç ´", "åˆ›æ–°", "PUE", "æ™ºç®—", "ç®—åŠ›", "AI", "å¤§æ¨¡å‹"],
            "æ”¿ç­–": ["æ”¿ç­–", "æ³•è§„", "æ ‡å‡†", "è§„èŒƒ", "å‘æ”¹å§”", "å·¥ä¿¡éƒ¨", "å›½åŠ¡é™¢"],
            "å¸‚åœº": ["å¸‚åœº", "ä»½é¢", "å¢é•¿", "æŠ¥å‘Š", "è§„æ¨¡", "è¶‹åŠ¿", "é¢„æµ‹"],
        }

        for category, keywords in categories.items():
            if any(kw in text for kw in keywords):
                return category

        return "å¸‚åœº"


def main():
    print("\n" + "=" * 80)
    print("å¤šåª’ä½“æºé›†æˆæµ‹è¯• - 4ä¸ªæ´»è·ƒæº")
    print("=" * 80)

    # Load config
    config_path = "config/media-sources.json"
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)

    # Get active sources
    active_sources = [s for s in config.get("sources", []) if s.get("active", False)]
    print(f"\nğŸ“Š å‘ç° {len(active_sources)} ä¸ªæ´»è·ƒåª’ä½“æº")
    for source in active_sources:
        print(f"   â€¢ {source.get('name')} (Tier {source.get('tier')})")

    # Initialize components
    scorer = SimpleScorer()
    classifier = SimpleClassifier()
    db = Database(db_path="tmp/multi_source_intelligence.db")

    # Collect from all sources
    all_articles = []
    source_stats = {}

    for source_config in active_sources:
        source_name = source_config.get("name")
        source_tier = source_config.get("tier", 2)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“¥ æŠ“å–: {source_name}")
        print('='*80)

        try:
            scraper = ScraperFactory.create_scraper(source_config)
            articles = scraper.fetch_articles(limit=5)
            
            print(f"   âœ… æˆåŠŸæŠ“å– {len(articles)} ç¯‡æ–‡ç« ")
            
            source_stats[source_name] = {
                "fetched": len(articles),
                "stored": 0,
                "duplicates": 0
            }
            
            # Process articles
            for article in articles:
                # Score
                score_result = scorer.score_article(
                    title=article["title"],
                    content=article.get("summary", ""),
                    publish_date=article.get("publish_date"),
                    source_tier=source_tier,
                )

                # Classify
                category = classifier.classify(
                    title=article["title"],
                    content=article.get("summary", "")
                )

                # Store
                article_id = db.insert_article(
                    title=article["title"],
                    url=article["url"],
                    source=article["source"],
                    publish_date=article.get("publish_date"),
                    summary=article.get("summary", ""),
                    content="",
                    score=score_result["total_score"],
                    priority=score_result["priority"],
                    category=category,
                    score_relevance=score_result["dimension_scores"]["relevance"],
                    score_timeliness=score_result["dimension_scores"]["timeliness"],
                    score_impact=score_result["dimension_scores"]["impact"],
                    score_credibility=score_result["dimension_scores"]["credibility"],
                    link_valid=True,
                )

                if article_id is None:
                    source_stats[source_name]["duplicates"] += 1
                else:
                    source_stats[source_name]["stored"] += 1
                    print(f"      [{score_result['priority']}] {article['title'][:50]}...")
                    print(f"         åˆ†æ•°: {score_result['total_score']} | åˆ†ç±»: {category}")

        except Exception as e:
            print(f"   âŒ æŠ“å–å¤±è´¥: {e}")
            source_stats[source_name] = {"fetched": 0, "stored": 0, "duplicates": 0, "error": str(e)}

    # Show summary
    print(f"\n{'='*80}")
    print("ğŸ“Š é‡‡é›†ç»Ÿè®¡")
    print('='*80)
    
    total_fetched = 0
    total_stored = 0
    total_duplicates = 0
    
    for source_name, stats in source_stats.items():
        fetched = stats.get("fetched", 0)
        stored = stats.get("stored", 0)
        duplicates = stats.get("duplicates", 0)
        
        total_fetched += fetched
        total_stored += stored
        total_duplicates += duplicates
        
        status = "âœ…" if fetched > 0 else "âŒ"
        print(f"\n{status} {source_name}:")
        print(f"   æŠ“å–: {fetched} | å­˜å‚¨: {stored} | é‡å¤: {duplicates}")
        if "error" in stats:
            print(f"   é”™è¯¯: {stats['error'][:60]}...")

    print(f"\n{'='*80}")
    print(f"æ€»è®¡: æŠ“å– {total_fetched} | å­˜å‚¨ {total_stored} | é‡å¤ {total_duplicates}")

    # Database statistics
    print(f"\n{'='*80}")
    print("ğŸ“Š æ•°æ®åº“ç»Ÿè®¡")
    print('='*80)

    all_db_articles = db.get_all_articles()
    print(f"\næ€»æ–‡ç« æ•°: {len(all_db_articles)}")

    # Priority distribution
    priority_counts = {}
    for article in all_db_articles:
        priority = article.get("priority", "æœªåˆ†ç±»")
        priority_counts[priority] = priority_counts.get(priority, 0) + 1

    print(f"\nä¼˜å…ˆçº§åˆ†å¸ƒ:")
    for priority in ["é«˜", "ä¸­", "ä½"]:
        count = priority_counts.get(priority, 0)
        print(f"   {priority}: {count}")

    # Category distribution
    category_counts = {}
    for article in all_db_articles:
        category = article.get("category", "æœªåˆ†ç±»")
        category_counts[category] = category_counts.get(category, 0) + 1

    print(f"\nåˆ†ç±»åˆ†å¸ƒ:")
    for category, count in sorted(category_counts.items(), key=lambda x: -x[1]):
        print(f"   {category}: {count}")

    # Source distribution
    source_counts = {}
    for article in all_db_articles:
        source = article.get("source", "æœªçŸ¥")
        source_counts[source] = source_counts.get(source, 0) + 1

    print(f"\næ¥æºåˆ†å¸ƒ:")
    for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):
        print(f"   {source}: {count}")

    # Show high-priority articles
    print(f"\n{'='*80}")
    print("ğŸŒŸ é«˜ä¼˜å…ˆçº§æ–‡ç« ")
    print('='*80)

    high_priority = db.get_articles_by_priority("é«˜")
    if high_priority:
        for i, article in enumerate(high_priority[:10], 1):
            print(f"\n{i}. [{article['score']}åˆ†] {article['title']}")
            print(f"   æ¥æº: {article['source']} | åˆ†ç±»: {article['category']} | æ—¥æœŸ: {article['publish_date']}")
    else:
        print("   æš‚æ— é«˜ä¼˜å…ˆçº§æ–‡ç« ")

    # Show medium-priority samples
    print(f"\n{'='*80}")
    print("ğŸ“° ä¸­ä¼˜å…ˆçº§æ–‡ç« ç¤ºä¾‹")
    print('='*80)

    medium_priority = db.get_articles_by_priority("ä¸­")
    if medium_priority:
        for i, article in enumerate(medium_priority[:5], 1):
            print(f"\n{i}. [{article['score']}åˆ†] {article['title'][:60]}...")
            print(f"   æ¥æº: {article['source']} | åˆ†ç±»: {article['category']}")

    print(f"\n{'='*80}")
    print("âœ… æµ‹è¯•å®Œæˆ!")
    print(f"ğŸ“ æ•°æ®åº“: tmp/multi_source_intelligence.db")
    print('='*80 + "\n")


if __name__ == "__main__":
    main()
