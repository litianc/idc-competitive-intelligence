"""
RSSè®¢é˜…çˆ¬è™«

ä¼˜å…ˆä½¿ç”¨RSSæ–¹å¼æŠ“å–æ–°é—»ï¼Œæ›´ç¨³å®šå¯é 
"""

import requests
import xml.etree.ElementTree as ET
from datetime import datetime, date
import sys
import os
import time
import random
from html import unescape

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.storage.database import Database


class RSSFeedScraper:
    """RSSè®¢é˜…çˆ¬è™«"""

    def __init__(self):
        self.user_agent = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"

    def get_headers(self):
        """è·å–è¯·æ±‚å¤´"""
        return {
            "User-Agent": self.user_agent,
            "Accept": "application/rss+xml, application/xml, text/xml",
        }

    def fetch_rss(self, url, max_items=20):
        """
        æŠ“å–RSS feed

        Args:
            url: RSSè®¢é˜…åœ°å€
            max_items: æœ€å¤šè·å–çš„æ¡ç›®æ•°

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        print(f"\næ­£åœ¨è·å–RSS: {url}")

        try:
            response = requests.get(url, headers=self.get_headers(), timeout=10)
            response.encoding = "utf-8"

            if response.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                return []

            # è§£æXML
            root = ET.fromstring(response.content)

            # åˆ¤æ–­RSSæ ¼å¼ï¼ˆRSS 2.0 æˆ– Atomï¼‰
            if root.tag == "rss":
                return self._parse_rss2(root, max_items)
            elif "feed" in root.tag.lower():
                return self._parse_atom(root, max_items)
            else:
                print(f"âŒ æœªçŸ¥çš„RSSæ ¼å¼: {root.tag}")
                return []

        except Exception as e:
            print(f"âŒ æŠ“å–RSSå¤±è´¥: {e}")
            return []

    def _parse_rss2(self, root, max_items):
        """è§£æRSS 2.0æ ¼å¼"""
        articles = []

        channel = root.find("channel")
        if not channel:
            return articles

        items = channel.findall("item")[:max_items]

        for item in items:
            try:
                title_elem = item.find("title")
                link_elem = item.find("link")
                pubdate_elem = item.find("pubDate")
                description_elem = item.find("description")

                if not title_elem or not link_elem:
                    continue

                title = unescape(title_elem.text or "")
                link = link_elem.text or ""
                description = unescape(description_elem.text or "") if description_elem else ""

                # è§£ææ—¥æœŸ
                publish_date = self._parse_date(pubdate_elem.text if pubdate_elem else "")

                article = {
                    "title": title.strip(),
                    "url": link.strip(),
                    "content": description.strip(),
                    "publish_date": publish_date,
                }

                articles.append(article)

            except Exception as e:
                print(f"  âœ— è§£ææ¡ç›®å¤±è´¥: {e}")
                continue

        return articles

    def _parse_atom(self, root, max_items):
        """è§£æAtomæ ¼å¼"""
        articles = []

        # Atomå‘½åç©ºé—´
        ns = {"atom": "http://www.w3.org/2005/Atom"}

        entries = root.findall("atom:entry", ns)[:max_items]

        for entry in entries:
            try:
                title_elem = entry.find("atom:title", ns)
                link_elem = entry.find("atom:link", ns)
                published_elem = entry.find("atom:published", ns)
                summary_elem = entry.find("atom:summary", ns)

                if not title_elem or not link_elem:
                    continue

                title = unescape(title_elem.text or "")
                link = link_elem.get("href", "")
                summary = unescape(summary_elem.text or "") if summary_elem else ""

                publish_date = self._parse_date(published_elem.text if published_elem else "")

                article = {
                    "title": title.strip(),
                    "url": link.strip(),
                    "content": summary.strip(),
                    "publish_date": publish_date,
                }

                articles.append(article)

            except Exception as e:
                print(f"  âœ— è§£ææ¡ç›®å¤±è´¥: {e}")
                continue

        return articles

    def _parse_date(self, date_str):
        """
        è§£ææ—¥æœŸå­—ç¬¦ä¸²

        æ”¯æŒå¤šç§æ ¼å¼ï¼š
        - RFC 822: Mon, 03 Nov 2025 14:30:00 +0800
        - ISO 8601: 2025-11-03T14:30:00+08:00
        """
        if not date_str:
            return date.today()

        try:
            # å°è¯•RFC 822æ ¼å¼ï¼ˆRSS 2.0ï¼‰
            dt = datetime.strptime(date_str[:25], "%a, %d %b %Y %H:%M:%S")
            return dt.date()
        except:
            pass

        try:
            # å°è¯•ISO 8601æ ¼å¼ï¼ˆAtomï¼‰
            dt = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
            return dt.date()
        except:
            pass

        # é»˜è®¤è¿”å›ä»Šå¤©
        return date.today()


# å¸¸è§çš„RSS feedåœ°å€ï¼ˆéœ€è¦å®é™…éªŒè¯ï¼‰
RSS_FEEDS = {
    "ä¸­å›½IDCåœˆ": "https://www.idcquan.com/rss.xml",
    "æ•°æ®ä¸­å¿ƒä¸–ç•Œ": "https://www.dcworld.cn/rss.xml",
    "é€šä¿¡ä¸–ç•Œç½‘": "https://www.cww.net.cn/rss.xml",
}


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("RSSè®¢é˜…çˆ¬è™«æ¼”ç¤º")
    print("=" * 70)

    scraper = RSSFeedScraper()

    # å°è¯•æŠ“å–å„ä¸ªRSSæº
    all_articles = []

    for source, rss_url in RSS_FEEDS.items():
        print(f"\nå°è¯•æŠ“å–: {source}")
        articles = scraper.fetch_rss(rss_url, max_items=10)

        if articles:
            print(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« ")
            for article in articles:
                article["source"] = source
                article["source_tier"] = 1 if source == "ä¸­å›½IDCåœˆ" else 1 if source == "æ•°æ®ä¸­å¿ƒä¸–ç•Œ" else 2
                all_articles.append(article)
        else:
            print(f"âš ï¸  {source} RSSå¯èƒ½ä¸å¯ç”¨ï¼Œè·³è¿‡")

        # ç¤¼è²Œå»¶è¿Ÿ
        time.sleep(random.uniform(1, 2))

    if not all_articles:
        print("\nâš ï¸  æœªèƒ½é€šè¿‡RSSè·å–åˆ°æ–‡ç« ")
        print("\nğŸ’¡ å»ºè®®ï¼š")
        print("1. æ£€æŸ¥ç½‘ç«™æ˜¯å¦æä¾›RSSè®¢é˜…")
        print("2. æŸ¥æ‰¾RSS feedçš„å®é™…URL")
        print("3. è€ƒè™‘ä½¿ç”¨ç½‘ç«™æä¾›çš„API")
        print("4. å¦‚æœéƒ½ä¸å¯ç”¨ï¼Œå†è€ƒè™‘ç½‘é¡µçˆ¬è™«")
        return

    print(f"\n{'=' * 70}")
    print(f"æ€»å…±è·å– {len(all_articles)} ç¯‡æ–‡ç« ")
    print("=" * 70)

    # å±•ç¤ºéƒ¨åˆ†æ–‡ç« 
    print("\næ–‡ç« ç¤ºä¾‹:")
    for idx, article in enumerate(all_articles[:5], 1):
        print(f"\n[{idx}] {article['title']}")
        print(f"    æ¥æº: {article['source']} | æ—¥æœŸ: {article['publish_date']}")
        print(f"    é“¾æ¥: {article['url']}")


if __name__ == "__main__":
    main()
